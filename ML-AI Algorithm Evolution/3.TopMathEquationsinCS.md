# 24 Most Important Math Definitions in Data Science
## Complete Reference with Visualizations, Fundamentals, and Use Cases

**Where Each Math Definition is Used in AI/ML/DL/NN**

This document provides deep explanations, visualizations, and specific use cases for the 24 most critical mathematical concepts in data science, organized by their application across Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL).

---

## Table of Contents

**[Optimization & Learning](#optimization--learning)**
1. [Gradient Descent](#1-gradient-descent)
2. [Lagrange Multiplier](#2-lagrange-multiplier)
3. [SVM Objective Function](#3-svm-objective-function)

**[Probability & Statistics](#probability--statistics)**

4. [Normal Distribution](#4-normal-distribution)
5. [Z-Score](#5-z-score)
6. [Naive Bayes](#6-naive-bayes)
7. [Maximum Likelihood Estimation (MLE)](#7-maximum-likelihood-estimation-mle)
8. [Entropy](#8-entropy)
9. [KL Divergence](#9-kl-divergence)

**[Linear Models](#linear-models)**

10. [Ordinary Least Squares (OLS)](#10-ordinary-least-squares-ols)
11. [Linear Regression](#11-linear-regression)
12. [Mean Squared Error (MSE)](#12-mean-squared-error-mse)
13. [MSE + L2 Regularization (Ridge)](#13-mse--l2-regularization-ridge)
14. [Log Loss](#14-log-loss)

**[Neural Networks](#neural-networks)**

15. [Sigmoid Function](#15-sigmoid-function)
16. [ReLU](#16-relu)
17. [Softmax](#17-softmax)
18. [Gradient Descent (in NN context)](#18-gradient-descent-in-nn-context)

**[Similarity & Distance](#similarity--distance)**

19. [Correlation](#19-correlation)
20. [Cosine Similarity](#20-cosine-similarity)
21. [K-Means Objective](#21-k-means-objective)

**[Model Evaluation](#model-evaluation)**

22. [F1 Score](#22-f1-score)
23. [R² Score](#23-r²-score)

**[Linear Algebra / Dimensionality Reduction](#linear-algebra--dimensionality-reduction)**

24. [Eigenvectors/Eigenvalues](#24-eigenvectorseigenvalues)
25. [Singular Value Decomposition (SVD)](#25-singular-value-decomposition-svd)

---

# Optimization & Learning

## 1. Gradient Descent

### **The Math**
```
θⱼ₊₁ = θⱼ - α∇J(θⱼ)
```

### **Fundamental Explanation**
Imagine you're blindfolded on a mountain and want to reach the valley (minimum). You:
1. Feel the slope under your feet (compute gradient)
2. Take a step downhill (multiply by learning rate)
3. Repeat until you can't go lower

The gradient ∇J points uphill, so we subtract it to go downhill.

### **Key Components**
- **θⱼ**: Current parameter values
- **α**: Learning rate (step size) - too big = overshoot, too small = slow
- **∇J(θⱼ)**: Gradient (direction of steepest ascent)
- **J(θ)**: Loss function (what we're minimizing)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Training linear regression | Find coefficients that minimize prediction error | Predicting house prices |
| **ML** | Training logistic regression | Find decision boundary | Email spam classification |
| **DL** | Training neural networks (backpropagation) | Adjust millions of weights | Image classification with CNNs |
| **DL** | Fine-tuning LLMs | Update model parameters on new data | Fine-tuning GPT for specific tasks |
| **RL** | Policy gradient methods | Improve agent's decision-making | Training game-playing AI |

### **Variants & Advanced Concepts**
- **Stochastic Gradient Descent (SGD)**: Use one sample at a time (faster but noisier)
- **Mini-batch GD**: Use small batches (balance speed and stability)
- **Adam, RMSProp**: Adaptive learning rates per parameter
- **Momentum**: Add "physics" to smooth out oscillations

### **When to Use**
- ✅ Any differentiable loss function
- ✅ Large datasets (use SGD/mini-batch)
- ✅ Deep learning (only practical optimization method)
- ❌ Non-differentiable functions (use genetic algorithms, etc.)

---

## 2. Lagrange Multiplier

### **The Math**
```
L(x, λ) = f(x) - λg(x)
where g(x) = 0 is the constraint
```

### **Fundamental Explanation**
You want to maximize profit f(x) but you have a budget constraint g(x) = 0. The Lagrange multiplier λ tells you:
- How much profit would increase if you relaxed the constraint slightly
- Acts as a "shadow price" of the constraint

**Example**: Maximize utility given budget constraint.

### **Key Components**
- **f(x)**: Objective function (what you want to optimize)
- **g(x) = 0**: Equality constraint
- **λ**: Lagrange multiplier (measures constraint sensitivity)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Support Vector Machines (SVM) | Maximize margin subject to classification constraints | Binary classification |
| **ML** | Resource allocation | Optimize performance under budget/time constraints | Cloud computing resource allocation |
| **RL** | Constrained policy optimization | Safe RL with safety constraints | Robotics with safety limits |
| **Optimization** | Portfolio optimization | Maximize returns subject to risk limits | Financial trading algorithms |

### **When to Use**
- ✅ Optimization problems with equality constraints
- ✅ When you need to understand constraint sensitivity
- ✅ Theoretical analysis of optimization
- ❌ For inequality constraints (use KKT conditions instead)

---

## 3. SVM Objective Function

### **The Math**
```
min (1/2)||w||² + C Σ max(0, 1 - yᵢ(w·xᵢ - b))
```

### **Fundamental Explanation**
SVM finds the widest possible "street" between two classes. The optimization balances:
1. **Maximize margin**: Make the street as wide as possible (minimize ||w||²)
2. **Minimize violations**: Penalize points on the wrong side or inside the margin

Think of it like building a highway with the widest median strip, but allowing some cars to cross over (with penalties).

### **Key Components**
- **||w||²**: Margin width (smaller w = wider margin)
- **C**: Penalty parameter (high C = strict, low C = more violations allowed)
- **yᵢ**: True label (+1 or -1)
- **max(0, ...)**: Hinge loss (penalty for violations)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Binary classification | Works well with high-dimensional data | Text classification |
| **ML** | Kernel SVM for non-linear boundaries | Can handle complex decision boundaries | Face recognition |
| **ML** | One-class SVM | Anomaly detection | Fraud detection, network intrusion |
| **ML** | Ranking problems | Learn to rank documents | Search engine result ordering |

### **When to Use**
- ✅ High-dimensional data (text, genomics)
- ✅ When you need maximum margin separation
- ✅ Binary classification with clear separation
- ❌ Very large datasets (slow to train)
- ❌ Overlapping classes with no clear boundary

---

# Probability & Statistics

## 4. Normal Distribution

### **The Math**
```
f(x|μ, σ²) = (1/σ√2π) exp(-(x-μ)²/2σ²)
```

### **Fundamental Explanation**
The bell curve appears everywhere in nature because of the **Central Limit Theorem**: when you add many independent random variables, their sum approaches a normal distribution.

- **μ (mean)**: Center of the bell
- **σ (standard deviation)**: Width of the bell
- **68-95-99.7 rule**: 68% within 1σ, 95% within 2σ, 99.7% within 3σ

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Gaussian Naive Bayes | Assumes features follow normal distribution | Medical diagnosis |
| **ML** | Gaussian Mixture Models (GMM) | Clustering with probabilistic membership | Customer segmentation |
| **DL** | Weight initialization | Initialize neural network weights | All deep learning models |
| **DL** | Batch Normalization | Normalize activations to improve training | CNNs, transformers |
| **RL** | Policy distributions | Model continuous action spaces | Robot control |
| **Stats** | Hypothesis testing | Foundation for t-tests, ANOVA | A/B testing |

### **When to Use**
- ✅ Modeling naturally occurring continuous variables
- ✅ When Central Limit Theorem applies
- ✅ Anomaly detection (outliers beyond 3σ)
- ❌ Highly skewed data (use log-normal, etc.)
- ❌ Discrete data (use Poisson, binomial)

---

## 5. Z-Score

### **The Math**
```
z = (x - μ) / σ
```

### **Fundamental Explanation**
Z-score answers: "How many standard deviations away from average is this value?"

- **z = 0**: Exactly average
- **z = 2**: Two standard deviations above average (unusual, top ~2.5%)
- **z = -3**: Three standard deviations below average (very unusual, bottom ~0.15%)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Feature scaling/standardization | Put all features on same scale | Preprocessing for SVM, neural networks |
| **ML** | Anomaly detection | Flag values beyond ±3σ as anomalies | Fraud detection, sensor fault detection |
| **Stats** | Outlier detection | Identify unusual data points | Data cleaning |
| **Finance** | Risk assessment | Standardize returns across assets | Portfolio risk management |

### **When to Use**
- ✅ Before training ML models (especially SVM, k-NN, neural networks)
- ✅ Comparing values from different scales
- ✅ Detecting outliers
- ❌ When distributions are not normal (use rank-based methods)

---

## 6. Naive Bayes

### **The Math**
```
P(y|x₁,...,xₙ) ∝ P(y) ∏ P(xᵢ|y)
```

### **Fundamental Explanation**

Bayes' theorem with a "naive" independence assumption:
1. Start with prior probability of each class P(y)
2. Multiply by likelihood of each feature given that class P(xᵢ|y)
3. Pick the class with highest posterior probability

**"Naive"** because it assumes features are independent (rarely true, but works anyway!).

### **Key Components**
- **P(y)**: Prior probability (how common is each class?)
- **P(xᵢ|y)**: Likelihood (how common is this feature in this class?)
- **P(y|x₁...xₙ)**: Posterior probability (updated belief after seeing features)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Text classification | Fast, works well with bag-of-words | Spam filtering, sentiment analysis |
| **ML** | Real-time classification | Very fast prediction | Email routing, document categorization |
| **ML** | Medical diagnosis | Interpretable probabilistic predictions | Disease classification from symptoms |
| **NLP** | Language detection | Simple feature independence assumption fits | Detecting language of text |

### **When to Use**
- ✅ Text classification (spam, sentiment, topics)
- ✅ When you need fast training and prediction
- ✅ When you have small training data
- ✅ When features are roughly independent
- ❌ When feature correlations are critical
- ❌ When you need state-of-the-art accuracy

---

## 7. Maximum Likelihood Estimation (MLE)

### **The Math**
```
θ̂ = arg max ∏ P(xᵢ|θ)
      θ    i=1
```

### **Fundamental Explanation**
You observed some data. MLE finds the parameters that make your observed data most probable.

**Example**: You flip a coin 10 times, get 7 heads. MLE says the probability of heads is 0.7 (makes your observation most likely).

### **Key Components**
- **θ**: Parameters we're trying to estimate
- **xᵢ**: Observed data
- **P(xᵢ|θ)**: Probability of seeing data given parameters
- **∏**: Product (multiply all probabilities)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Logistic regression | Find coefficients | Binary classification |
| **ML** | Gaussian Mixture Models | Estimate mixture parameters | Clustering |
| **ML** | Hidden Markov Models | Estimate transition/emission probabilities | Speech recognition |
| **Stats** | Parameter estimation | Foundation of statistical inference | Any parametric model |
| **DL** | Training probabilistic models | Cross-entropy loss is MLE | Classification networks |

### **When to Use**
- ✅ Fitting probabilistic models
- ✅ When you can write down likelihood function
- ✅ Parameter estimation in statistics
- ❌ When likelihood is intractable (use variational methods)

---

## 8. Entropy

### **The Math**
```
H(X) = -Σ p(xᵢ) log p(xᵢ)
```

### **Fundamental Explanation**
Entropy measures unpredictability or "surprise":
- **Low entropy**: Very predictable (e.g., coin with two heads)
- **High entropy**: Very unpredictable (e.g., fair coin, dice)

**Information theory**: Entropy tells you the average number of bits needed to encode outcomes.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Decision trees (ID3, C4.5) | Choose best feature to split on | Random forests, XGBoost |
| **ML** | Information gain | Measure feature importance | Feature selection |
| **DL** | Cross-entropy loss | Measure prediction uncertainty | Neural network classification |
| **RL** | Entropy regularization | Encourage exploration | SAC, PPO with entropy bonus |
| **Info Theory** | Data compression | Determine optimal encoding | Huffman coding |

### **When to Use**
- ✅ Building decision trees
- ✅ Measuring disorder/uncertainty
- ✅ Encouraging exploration in RL
- ✅ Information-theoretic analysis

---

## 9. KL Divergence

### **The Math**
```
D_KL(P||Q) = Σ P(x) log(P(x)/Q(x))
```

### **Fundamental Explanation**
KL Divergence measures "how different" two probability distributions are:
- **D_KL = 0**: Distributions are identical
- **D_KL > 0**: Distributions differ (larger = more different)
- **Not symmetric**: D_KL(P||Q) ≠ D_KL(Q||P)

Think of it as: "Extra bits needed to encode P using code designed for Q".

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **DL** | Variational Autoencoders (VAE) | Force latent space to match prior distribution | Generative models, image generation |
| **DL** | Knowledge distillation | Match student model to teacher model | Model compression |
| **RL** | Policy optimization (TRPO, PPO) | Constrain policy updates | Stable policy learning |
| **RL** | Inverse reinforcement learning | Match agent distribution to expert | Imitation learning |
| **ML** | Model selection | Compare probabilistic models | Bayesian model comparison |

### **When to Use**
- ✅ Comparing probability distributions
- ✅ Variational inference
- ✅ Constrained policy updates in RL
- ❌ When you need symmetric distance (use JS divergence)

---

# Linear Models

## 10. Ordinary Least Squares (OLS)

### **The Math**
```
β̂ = (XᵀX)⁻¹Xᵀy
```

### **Fundamental Explanation**
OLS finds the best-fit line by minimizing the sum of squared vertical distances from points to the line.

- **Closed-form solution**: No iteration needed, just matrix operations
- **Geometric interpretation**: Project y onto the column space of X

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Linear regression | Simple, interpretable predictions | House price prediction |
| **Stats** | Hypothesis testing | Foundation of statistical inference | t-tests on coefficients |
| **Economics** | Econometric models | Policy analysis | Effect of education on income |
| **Finance** | Beta calculation | Measure stock sensitivity to market | Portfolio risk analysis |

### **When to Use**
- ✅ Linear relationships
- ✅ When you need exact solution
- ✅ Small to medium datasets
- ✅ When interpretability matters
- ❌ Multicollinearity (features highly correlated)
- ❌ More features than samples (use Ridge/Lasso)

---

## 11. Linear Regression

### **The Math**
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

### **Fundamental Explanation**
Model output as a weighted sum of inputs plus noise:
- **β₀**: Intercept (baseline value when all x=0)
- **βᵢ**: Slope for feature i (change in y per unit change in xᵢ)
- **ε**: Random error (noise)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Regression tasks | Baseline model, interpretable | Sales forecasting, pricing |
| **ML** | Feature importance | Coefficient magnitudes show importance | Understanding drivers of outcome |
| **Finance** | Factor models | Decompose returns into factors | Fama-French model |
| **Science** | Modeling relationships | Test hypotheses about relationships | Clinical trials |

### **When to Use**
- ✅ First model to try (baseline)
- ✅ Linear relationships
- ✅ Need interpretable coefficients
- ❌ Non-linear relationships (use polynomial, splines)
- ❌ Categorical outputs (use logistic regression)

---

## 12. Mean Squared Error (MSE)

### **The Math**
```
MSE = (1/n) Σ(yᵢ - ŷᵢ)²
```

### **Fundamental Explanation**
Average of squared prediction errors:
1. Calculate error for each prediction: (actual - predicted)
2. Square it (so negative errors don't cancel positive)
3. Average across all samples

**Why square?** 
- Penalizes large errors more than small errors
- Mathematically convenient (differentiable)
- Corresponds to Gaussian noise assumption

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Regression loss function | Standard loss for continuous outputs | All regression models |
| **DL** | Training neural networks | Default loss for regression | Image super-resolution, time series |
| **RL** | Value function fitting | Minimize TD error | Q-learning, DQN |
| **Stats** | Model evaluation | Compare model performance | Model selection |

### **When to Use**
- ✅ Regression problems
- ✅ When large errors should be heavily penalized
- ✅ When errors are normally distributed
- ❌ When outliers are common (use MAE, Huber loss)
- ❌ When errors have different scales (use percentage errors)

---

## 13. MSE + L2 Regularization (Ridge)

### **The Math**
```
MSE_reg = (1/n)Σ(yᵢ - ŷᵢ)² + λΣβⱼ²
```

### **Fundamental Explanation**
Ridge adds a penalty for large coefficients:
- **First term**: Standard MSE (fit data well)
- **Second term**: L2 penalty (keep coefficients small)
- **λ**: Controls trade-off (high λ = more regularization)

**Why?** Prevents overfitting by discouraging complex models.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | High-dimensional regression | Handles multicollinearity | Genomics, text features |
| **ML** | Preventing overfitting | Reduces model complexity | Any regression with many features |
| **DL** | Weight decay | Standard regularization in neural networks | All deep learning |
| **Finance** | Portfolio optimization | Stabilize estimates | Mean-variance optimization |

### **When to Use**
- ✅ More features than samples
- ✅ Multicollinearity (correlated features)
- ✅ Preventing overfitting
- ❌ Need exact feature selection (use Lasso instead)
- ❌ Features on very different scales (standardize first)

---

## 14. Log Loss

### **The Math**
```
Loss = -(1/N)Σ[yᵢlog(ŷᵢ) + (1-yᵢ)log(1-ŷᵢ)]
```

### **Fundamental Explanation**
Log loss (cross-entropy) heavily penalizes confident wrong predictions:
- **Predict 0.9 for class 1**: Small loss (good!)
- **Predict 0.1 for class 1**: Huge loss (very bad!)
- **Predict 0.5**: Moderate loss (uncertain)

Encourages calibrated probabilities.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Logistic regression | Standard loss for binary classification | Click-through rate prediction |
| **DL** | Neural network classification | Default loss for classification | Image classification, NLP |
| **DL** | Language modeling | Predict next token | GPT, BERT training |
| **RL** | Policy gradient methods | Maximize expected reward | Discrete action spaces |

### **When to Use**
- ✅ Binary classification
- ✅ Multi-class classification (use softmax + cross-entropy)
- ✅ When you need calibrated probabilities
- ❌ Imbalanced classes without weighting
- ❌ Noisy labels (use focal loss)

---

# Neural Networks

## 15. Sigmoid Function

### **The Math**
```
σ(x) = 1 / (1 + e⁻ˣ)
```

### **Fundamental Explanation**
Sigmoid squashes any real number to [0, 1]:
- **Large positive x**: Output → 1
- **Large negative x**: Output → 0
- **x = 0**: Output = 0.5

Perfect for converting scores to probabilities.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Logistic regression | Output layer for binary classification | Email spam detection |
| **DL** | Binary classification output | Final layer activation | Medical diagnosis (disease/no disease) |
| **DL** | LSTM gates | Control information flow | Sequence modeling |
| **Historical** | Hidden layer activation (outdated) | Non-linearity | Early neural networks (now use ReLU) |

### **When to Use**
- ✅ Binary classification output layer
- ✅ When output must be [0,1]
- ✅ LSTM/GRU gates
- ❌ Hidden layers (use ReLU - faster, no vanishing gradient)
- ❌ Multi-class (use softmax instead)

---

## 16. ReLU

### **The Math**
```
f(x) = max(0, x)
```

### **Fundamental Explanation**
The simplest non-linear activation:
- **x > 0**: Output = x (keep positive values)
- **x ≤ 0**: Output = 0 (kill negative values)

**Why it dominates**:
- Fast to compute (just one comparison)
- No vanishing gradient for positive values
- Sparse activation (many neurons output 0)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **DL** | Hidden layers in CNNs | Default activation | Image classification (AlexNet, ResNet, VGG) |
| **DL** | Hidden layers in MLPs | Default activation | Tabular data, dense networks |
| **DL** | Transformers | FFN activation | BERT, GPT, ViT |
| **RL** | Value/policy networks | Hidden layer activation | DQN, PPO, A3C |

### **When to Use**
- ✅ Hidden layers in almost all neural networks
- ✅ When you need fast training
- ✅ Default choice for deep learning
- ❌ Output layer (use sigmoid/softmax/linear)
- ❌ When dying ReLU is a problem (use Leaky ReLU)

---

## 17. Softmax

![Softmax](./math_visualizations/13_softmax.png)

### **The Math**
```
P(y=j|x) = e^(xⱼ) / Σₖe^(xₖ)
```

### **Fundamental Explanation**
Softmax converts raw scores (logits) into probabilities:
1. Exponentiate each score (makes them positive)
2. Divide by sum (makes them sum to 1)
3. Now you have a probability distribution

**Example**: Scores [2.0, 1.0, 0.1] → Probabilities [0.66, 0.24, 0.10]

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **DL** | Multi-class classification | Output layer | Image classification (ImageNet: 1000 classes) |
| **NLP** | Language modeling | Predict next token | GPT, BERT |
| **NLP** | Machine translation | Choose next word | Seq2seq models |
| **RL** | Discrete action spaces | Policy network output | Atari games |
| **CV** | Object detection | Class probabilities per bounding box | YOLO, Faster R-CNN |

### **When to Use**
- ✅ Multi-class classification (mutually exclusive classes)
- ✅ When you need probability distribution
- ✅ Output layer for categorical outcomes
- ❌ Binary classification (use sigmoid)
- ❌ Multi-label classification (use multiple sigmoids)

---

## 18. Gradient Descent in NN Context

### **Deep Learning Specific Applications**

| Application | Details |
|------------|---------|
| **Backpropagation** | Compute gradients layer-by-layer using chain rule |
| **Weight updates** | Update millions of parameters simultaneously |
| **Stochastic variants** | SGD, Mini-batch GD for large datasets |
| **Adaptive optimizers** | Adam, RMSProp automatically adjust learning rates |
| **Learning rate schedules** | Decay learning rate during training |

### **Critical in DL**
- Only practical way to train deep networks
- Backpropagation = efficient gradient computation
- Adam is default optimizer for most modern architectures

---

# Similarity & Distance

## 19. Correlation

### **The Math**
```
Corr(X,Y) = Cov(X,Y) / (Std(X) · Std(Y))
```

### **Fundamental Explanation**
Correlation measures linear relationship between two variables:
- **+1**: Perfect positive (X up → Y up)
- **0**: No linear relationship
- **-1**: Perfect negative (X up → Y down)

**Important**: Correlation ≠ Causation!

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Feature selection | Remove highly correlated features | Dimensionality reduction |
| **ML** | Exploratory data analysis | Understand relationships | Initial data investigation |
| **Finance** | Portfolio diversification | Find uncorrelated assets | Risk management |
| **Stats** | Hypothesis testing | Test for relationships | Research studies |

### **When to Use**
- ✅ Exploratory data analysis
- ✅ Feature engineering
- ✅ Understanding data structure
- ❌ Non-linear relationships (use mutual information)
- ❌ Causality (use causal inference)

---

## 20. Cosine Similarity

### **The Math**
```
sim(A,B) = (A·B) / (||A|| ||B||)
```

### **Fundamental Explanation**
Cosine similarity measures the angle between two vectors (ignores magnitude):
- **1**: Vectors point in same direction (very similar)
- **0**: Vectors are perpendicular (unrelated)
- **-1**: Vectors point in opposite directions (opposite)

**Why ignore magnitude?** Document length shouldn't affect similarity.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **NLP** | Document similarity | Compare text regardless of length | Plagiarism detection, search |
| **NLP** | Word embeddings (word2vec) | Find similar words | "king" - "man" + "woman" = "queen" |
| **Recommender** | Item-item similarity | Find similar products | "Customers who bought X also liked Y" |
| **Recommender** | User-user similarity | Find similar users | Collaborative filtering |
| **CV** | Face recognition | Compare face embeddings | Facial recognition systems |

### **When to Use**
- ✅ High-dimensional sparse data (text)
- ✅ When magnitude is not meaningful
- ✅ Recommendation systems
- ❌ When magnitude matters (use Euclidean distance)

---

## 21. K-Means Objective

### **The Math**
```
min ΣᵢΣₓ∈Sᵢ ||x - μᵢ||²
```

### **Fundamental Explanation**
K-Means groups data into K clusters by:
1. Place K random centroids
2. Assign each point to nearest centroid
3. Move centroids to average of assigned points
4. Repeat until centroids stop moving

**Minimizes**: Total squared distance from points to their cluster centers.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Customer segmentation | Find natural customer groups | Marketing campaigns |
| **ML** | Image compression | Reduce color palette | GIF compression |
| **ML** | Anomaly detection | Points far from all centroids | Fraud detection |
| **CV** | Image segmentation | Group similar pixels | Medical image analysis |
| **NLP** | Topic clustering | Group similar documents | News article organization |

### **When to Use**
- ✅ When you know K (number of clusters)
- ✅ Spherical, roughly equal-sized clusters
- ✅ Fast, simple clustering
- ❌ Non-spherical clusters (use DBSCAN, GMM)
- ❌ Unknown K (use hierarchical clustering)

---

# Model Evaluation

## 22. F1 Score

### **The Math**
```
F1 = 2PR / (P + R)
where P = TP/(TP+FP), R = TP/(TP+FN)
```

### **Fundamental Explanation**
F1 balances precision and recall:
- **Precision**: Of predicted positives, how many are correct?
- **Recall**: Of actual positives, how many did we find?
- **F1**: Harmonic mean (punishes extreme imbalance)

**Example**: 
- High precision, low recall: Model is picky (few false positives)
- Low precision, high recall: Model is aggressive (few false negatives)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Imbalanced classification | Better than accuracy for skewed classes | Fraud detection (0.1% fraud) |
| **ML** | Medical diagnosis | Balance false positives vs false negatives | Cancer screening |
| **NLP** | Named entity recognition | Evaluate entity extraction | NER models |
| **CV** | Object detection | Evaluate detection quality | YOLO, R-CNN evaluation |

### **When to Use**
- ✅ Imbalanced classes
- ✅ When both FP and FN matter
- ✅ Binary classification evaluation
- ❌ Balanced classes (accuracy is fine)
- ❌ When FP and FN have different costs (use custom metric)

---

## 23. R² Score

### **The Math**
```
R² = 1 - Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²
```

### **Fundamental Explanation**
R² measures goodness-of-fit:
- **1**: Perfect predictions
- **0**: Model is no better than predicting the mean
- **Negative**: Model is worse than predicting the mean

**Interpretation**: R² = 0.8 means model explains 80% of variance.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Regression evaluation | Standard metric for regression | House price prediction |
| **Stats** | Model comparison | Compare different regression models | Model selection |
| **Finance** | Portfolio performance | Explain returns | Factor models |
| **Science** | Model validation | Assess model fit | Scientific modeling |

### **When to Use**
- ✅ Regression problems
- ✅ Comparing model performance
- ✅ Explaining variance
- ❌ Extrapolation (R² doesn't guarantee good predictions outside training range)
- ❌ Non-linear relationships (use adjusted R²)

---

# Linear Algebra / Dimensionality Reduction

## 24. Eigenvectors/Eigenvalues

### **The Math**
```
Av = λv
```

### **Fundamental Explanation**
Eigenvectors are special directions where a matrix only scales (doesn't rotate):
- **v**: Eigenvector (direction)
- **λ**: Eigenvalue (scaling factor)

**Example**: For a covariance matrix, eigenvectors show directions of maximum variance.

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Principal Component Analysis (PCA) | Find directions of maximum variance | Dimensionality reduction |
| **ML** | Spectral clustering | Use eigenvectors of graph Laplacian | Community detection |
| **DL** | Understanding neural networks | Analyze weight matrices | Model interpretability |
| **Physics** | Quantum mechanics | Eigenstates of operators | Quantum computing |
| **Graph** | PageRank | Dominant eigenvector of transition matrix | Web page ranking |

### **When to Use**
- ✅ Understanding linear transformations
- ✅ PCA for dimensionality reduction
- ✅ Spectral methods
- ❌ When matrix is not square
- ❌ For very large matrices (use approximate methods)

---

## 25. Singular Value Decomposition (SVD)

### **The Math**
```
A = UΣVᵀ
```

### **Fundamental Explanation**
SVD decomposes any matrix into three simpler matrices:
- **U**: Left singular vectors (row patterns)
- **Σ**: Singular values (importance of each pattern)
- **Vᵀ**: Right singular vectors (column patterns)

**Think of it as**: Principal components for rectangular matrices.

### **Key Components**
- **U**: Orthogonal matrix (m×m)
- **Σ**: Diagonal matrix of singular values (m×n)
- **Vᵀ**: Orthogonal matrix (n×n)

### **Use Cases by Domain**

| Domain | Specific Application | Why It's Used | Example |
|--------|---------------------|---------------|---------|
| **ML** | Dimensionality reduction | Generalization of PCA | Feature extraction |
| **Recommender** | Matrix factorization | Collaborative filtering | Netflix prize, movie recommendations |
| **NLP** | Latent Semantic Analysis (LSA) | Find document topics | Document clustering |
| **CV** | Image compression | Store only top singular values | JPEG-like compression |
| **DL** | Model compression | Compress weight matrices | Pruning neural networks |

### **When to Use**
- ✅ Collaborative filtering
- ✅ Dimensionality reduction for any matrix
- ✅ Missing data imputation
- ✅ Matrix approximation
- ❌ Very large matrices (use randomized SVD)
- ❌ When sparsity is critical (SVD creates dense matrices)

---

## Quick Reference: When to Use What

### **By Task Type**

| Task | Primary Methods |
|------|----------------|
| **Binary Classification** | Logistic Regression (Sigmoid + Log Loss), SVM |
| **Multi-class Classification** | Softmax + Cross-Entropy, Random Forest |
| **Regression** | Linear Regression (MSE), Ridge, Neural Networks |
| **Clustering** | K-Means, GMM (Normal Distribution) |
| **Dimensionality Reduction** | PCA (Eigenvectors), SVD, t-SNE |
| **Anomaly Detection** | Z-Score, One-class SVM, Isolation Forest |
| **Text Classification** | Naive Bayes, TF-IDF + Cosine Similarity |
| **Recommendation** | SVD, Cosine Similarity, Collaborative Filtering |

### **By Domain**

| Domain | Core Math Concepts |
|--------|-------------------|
| **Traditional ML** | OLS, Ridge, SVM, Naive Bayes, K-Means, PCA |
| **Deep Learning** | Gradient Descent, ReLU, Softmax, Log Loss, Batch Norm (Z-Score) |
| **NLP** | Cosine Similarity, Softmax, Entropy, SVD, Transformers |
| **Computer Vision** | CNN (ReLU, Softmax), Correlation, Eigenvectors |
| **Reinforcement Learning** | Gradient Descent, Entropy, KL Divergence, MSE (TD error) |
| **Time Series** | Linear Regression, LSTM (Sigmoid), AR models |

### **By Problem Characteristics**

| Characteristic | Recommended Approach |
|---------------|---------------------|
| **Small data** | Simple models (Linear, Naive Bayes), regularization (Ridge) |
| **Large data** | Deep Learning (Gradient Descent + Adam), Ensemble methods |
| **High dimensions** | Regularization (Ridge), PCA, Feature selection |
| **Imbalanced classes** | F1 Score, weighted loss, SMOTE |
| **Non-linear** | Kernel SVM, Deep Learning, Ensemble methods |
| **Need interpretability** | Linear models, Decision trees, correlation analysis |

---

## Summary: The Math Behind Modern AI

These 24 mathematical concepts form the complete foundation of data science:

1. **Optimization** (Gradient Descent, Lagrange) → How models learn
2. **Probability** (Normal, Bayes, MLE) → Reasoning under uncertainty
3. **Loss Functions** (MSE, Log Loss) → What models optimize
4. **Activation Functions** (Sigmoid, ReLU, Softmax) → Non-linearity in neural networks
5. **Similarity** (Correlation, Cosine) → Finding relationships
6. **Evaluation** (F1, R²) → Measuring success
7. **Linear Algebra** (Eigenvectors, SVD) → Understanding data structure

**Master these fundamentals, and you can:**
- Understand research papers
- Debug model training
- Choose appropriate methods
- Design custom architectures
- Explain model behavior

**Remember**: Every complex AI system is built by combining these simple mathematical building blocks in clever ways.
